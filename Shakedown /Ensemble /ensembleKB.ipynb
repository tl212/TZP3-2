{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2017\n",
    "np.random.seed(seed)\n",
    "\n",
    "data = load_iris()\n",
    "idx = np.random.permutation(150)\n",
    "X = data.data[idx]\n",
    "y = data.target[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building an ensemble\n",
    "Instantiating a fully specified ensemble is straightforward and requires three steps: first create the instance, second add the intermediate layers, and finally the meta estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "Processing layer-1             done | 00:00:00\n",
      "Processing layer-2             done | 00:00:00\n",
      "Fit complete                        | 00:00:00\n",
      "\n",
      "Predicting 2 layers\n",
      "Processing layer-1             done | 00:00:00\n",
      "Processing layer-2             done | 00:00:00\n",
      "Predict complete                    | 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# --- Build ---\n",
    "# Passing a scoring function will create cv scores during fitting\n",
    "# the scorer should be a simple function accepting to vectors and returning a scalar\n",
    "ensemble = SuperLearner(scorer=accuracy_score, random_state=seed, verbose=2)\n",
    "\n",
    "# Build the first layer\n",
    "ensemble.add([RandomForestClassifier(random_state=seed), SVC()])\n",
    "\n",
    "# Attach the final meta estimator \n",
    "ensemble.add_meta(LogisticRegression())\n",
    "\n",
    "# --- Use ---\n",
    "\n",
    "# Fit ensemble \n",
    "ensemble.fit(X[:75], y[:75])\n",
    "\n",
    "# Predict \n",
    "preds = ensemble.predict(X[75:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit data:\n",
      "                                   score-m  score-s  ft-m  ft-s  pt-m  pt-s\n",
      "layer-1  randomforestclassifier       0.84     0.06  0.04  0.00  0.00  0.00\n",
      "layer-1  svc                          0.89     0.05  0.00  0.00  0.00  0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit data:\\n%r\" % ensemble.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pandas.core.frame.DataFrame.count(self, axis=0, level=None, numeric_only=False)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<property at 0x11ea4df98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction score: 0.960\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction score: %.3f\" % accuracy_score(preds, y[75:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer ensembles\n",
    "With each call to the `add` method, another layer is added to the ensemble. Note that all ensembles are sequential in the order layers are added. For instance, in the above example, we could add a second layer as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=2,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
       "   random_state=9787, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, folds=2, raise_on_ex...816620>)],\n",
       "   n_jobs=-1, name='group-14', raise_on_exception=True, transformers=[])],\n",
       "   verbose=0)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=2017, sample_size=20,\n",
       "       scorer=<function accuracy_score at 0x1a21816620>, shuffle=False,\n",
       "       verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = SuperLearner(scorer=accuracy_score, random_state=seed)\n",
    "\n",
    "# Build the first layer\n",
    "ensemble.add([RandomForestClassifier(random_state=seed), LogisticRegression()])\n",
    "\n",
    "# Build the second layer\n",
    "ensemble.add([LogisticRegression(), SVC()])\n",
    "\n",
    "# Attach the final meta estimator\n",
    "ensemble.add_meta(SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit data:\n",
      "                                   score-m  score-s  ft-m  ft-s  pt-m  pt-s\n",
      "layer-1  logisticregression           0.75     0.14  0.02  0.01  0.00  0.00\n",
      "layer-1  randomforestclassifier       0.84     0.06  0.04  0.01  0.00  0.00\n",
      "layer-2  logisticregression           0.67     0.12  0.00  0.00  0.00  0.00\n",
      "layer-2  svc                          0.89     0.00  0.00  0.00  0.00  0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble.fit(X[:75], y[:75])\n",
    "preds = ensemble.predict(X[75:])\n",
    "print(\"Fit data:\\n%r\" % ensemble.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple evaluation\n",
    "Before throwing preprocessing into the mix, let’s see how to evaluate a set of estimator. First, we need a list of estimator and a dictionary of parameter distributions that maps to each estimator. The estimators should be put in a list, either as is or as a named tuple ((name, est)). If you don’t name the estimator, the Evaluator will automatically name the model as the class name in lower case. This name must be the key in the parameter dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.model_selection import Evaluator \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the estimators\n",
    "ests = [('gnb', GaussianNB()), ('knn', KNeighborsClassifier())]\n",
    "\n",
    "# Mapping parameters to these / Since gnb doesn't have any parameters we can just skip it \n",
    "pars = {'n_neighbors': randint(2, 20)}\n",
    "params = {'knn': pars}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run an evaluation over these estimators and parameter distributions by calling the fit method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job\n",
      "Job           done | 00:00:01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlens.model_selection.model_selection.Evaluator at 0x10f9665f8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = Evaluator(accuracy_scorer, cv=10, random_state=seed, verbose=1)\n",
    "evaluator.fit(X, y, ests, params, n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full history of the evaluation can be found in `cv_results`. To compare models with their best parameters, we can pass the `results` attribute to a pandas.DataFrame or print it as a table. We use `m` to denote mean values and `s` to denote standard deviation across folds for brevity. Note that the timed prediction is for the training set, for comparability with training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score comparison with best params founds:\n",
      "\n",
      "       test_score-m  test_score-s  train_score-m  train_score-s  fit_time-m  fit_time-s  pred_time-m  pred_time-s               params\n",
      "gnb           0.960         0.033          0.957          0.006       0.009       0.006        0.004        0.001                     \n",
      "knn           0.967         0.033          0.980          0.005       0.002       0.001        0.051        0.005  {'n_neighbors': 15}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Score comparison with best params founds:\\n\\n%r\" % evaluator.results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "Next, suppose we want to compare the models across a set of preprocessing pipelines. To do this, we first need to specify a dictionary of preprocessing pipelines to run through. Each entry in the dictionary should be a list of transformers to apply sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.preprocessing import Subset\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map preprocessing cases through a dictionary\n",
    "preprocess_cases = {'none': [],\n",
    "                    'sc': [StandardScaler()],\n",
    "                    'sub': [Subset([0, 1])]\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit methods determines automatically whether there is any preprocessing or any estimator jobs to run, so all we need to do is specify the arguments we want to be processed. If a previous preprocessing job was fitted, those pipelines are stored and will be used for subsequent estimator fits.\n",
    "\n",
    "This can be helpful if the preprocessing is time-consuming, for instance if the preprocessing pipeline is an ensemble itself. All ensembles implement a `transform` method that, in contrast to the predict method, regenerates the predictions made during the fit``call. More precisely, the ``transform method uses the estimators fitted with cross-validation to construct predictions, whereas the `predict` method uses the final estimators fitted on all data. This allows us use ensembles as preprocessing steps that mimicks how that ensemble would produce predictions for a subsequent meta learner or layer. Since fitting large ensembles is highly time-consuming, fixing the lower layers as preprocessing input is highly valuable for tuning the higher layers and / or the final meta learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job\n",
      "Job           done | 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlens.model_selection.model_selection.Evaluator at 0x10f9665f8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.fit(X, y, preprocessing=preprocess_cases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection across preprocessing pipelines\n",
    "To evaluate the same set of estimators across all pipelines with the same parameter distributions, there is no need to take any heed of the preprocessing pipeline, just carry on as in the simple case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job\n",
      "Job           done | 00:00:04\n",
      "\n",
      "Comparison across preprocessing pipelines:\n",
      "\n",
      "             test_score-m  test_score-s  train_score-m  train_score-s  fit_time-m  fit_time-s  pred_time-m  pred_time-s               params\n",
      "none  gnb           0.960         0.033          0.957          0.006       0.005       0.002        0.005        0.002                     \n",
      "none  knn           0.967         0.033          0.980          0.005       0.002       0.003        0.037        0.028  {'n_neighbors': 15}\n",
      "sc    gnb           0.960         0.033          0.957          0.006       0.006       0.002        0.005        0.002                     \n",
      "sc    knn           0.960         0.044          0.965          0.003       0.002       0.000        0.056        0.005   {'n_neighbors': 8}\n",
      "sub   gnb           0.780         0.133          0.791          0.020       0.005       0.002        0.003        0.001                     \n",
      "sub   knn           0.800         0.126          0.837          0.015       0.002       0.001        0.053        0.003   {'n_neighbors': 9}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator.fit(X, y, ests, params, n_iter=10)\n",
    "print(\"\\nComparison across preprocessing pipelines:\\n\\n%r\" % evaluator.results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
